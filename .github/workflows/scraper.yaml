name: News Scraper

on:
  # Run automatically every day at 8:00 AM UTC (2:00 PM Bangladesh Time)
  schedule:
    - cron: '0 8 * * *'  # Change this time as needed
  
  # Allow manual trigger from GitHub Actions tab
  workflow_dispatch:
  
  # Or trigger on push to main branch (for testing)
  # push:
  #   branches: [ main ]

jobs:
  scrape-news:
    runs-on: ubuntu-latest
    
    steps:
    # Step 1: Checkout the repository
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    # Step 2: Set up Python
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
    
    # Step 3: Cache dependencies for faster runs
    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    # Step 4: Install dependencies
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    # Step 5: Run the scraper
    - name: Run news scraper
      env:
        GITHUB_ACCESS_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        python main.py
    
    # Step 6: Commit and push changes (alternative to using GitHub API)
    - name: Commit and push if changes
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add -A
        git diff --quiet && git diff --staged --quiet || (git commit -m "Update scraped articles - $(date)" && git push)